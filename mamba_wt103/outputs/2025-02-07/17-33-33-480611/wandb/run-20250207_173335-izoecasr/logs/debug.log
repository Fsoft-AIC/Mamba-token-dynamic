2025-02-07 17:33:35,636 INFO    MainThread:29620 [wandb_setup.py:_flush():68] Current SDK version is 0.19.6
2025-02-07 17:33:35,636 INFO    MainThread:29620 [wandb_setup.py:_flush():68] Configure stats pid to 29620
2025-02-07 17:33:35,636 INFO    MainThread:29620 [wandb_setup.py:_flush():68] Loading settings from C:\Users\phamd\.config\wandb\settings
2025-02-07 17:33:35,636 INFO    MainThread:29620 [wandb_setup.py:_flush():68] Loading settings from D:\atFSOFTaic\2410xx\mamba_token_dynamic\mamba_wt103\outputs\2025-02-07\17-33-33-480611\wandb\settings
2025-02-07 17:33:35,636 INFO    MainThread:29620 [wandb_setup.py:_flush():68] Loading settings from environment variables
2025-02-07 17:33:35,637 INFO    MainThread:29620 [wandb_init.py:setup_run_log_directory():637] Logging user logs to .\wandb\run-20250207_173335-izoecasr\logs\debug.log
2025-02-07 17:33:35,637 INFO    MainThread:29620 [wandb_init.py:setup_run_log_directory():638] Logging internal logs to .\wandb\run-20250207_173335-izoecasr\logs\debug-internal.log
2025-02-07 17:33:35,638 INFO    MainThread:29620 [wandb_init.py:init():756] calling init triggers
2025-02-07 17:33:35,638 INFO    MainThread:29620 [wandb_init.py:init():761] wandb.init called with sweep_config: {}
config: {'train': {'seed': 42, 'interval': 'step', 'monitor': 'val/loss', 'mode': 'min', 'ema': 0.0, 'test': False, 'debug': False, 'ignore_warnings': False, 'state': {'mode': None, 'n_context': 0, 'n_context_eval': 0}, 'ckpt': None, 'disable_dataset': False, 'validate_at_start': False, 'pretrained_model_path': None, 'pretrained_model_strict_load': True, 'pretrained_model_state_hook': {'_name_': None}, 'post_init_hook': {'_name_': None}, 'layer_decay': {'_name_': None, 'decay': 0.7}, 'gpu_mem': 4, 'global_batch_size': 128}, 'tolerance': {'logdir': './resume', 'id': None}, 'wandb': {'project': 'long-conv', 'group': '', 'job_type': 'training', 'mode': 'online', 'name': None, 'save_dir': '.', 'id': None}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'devices': 1, 'accelerator': 'gpu', 'accumulate_grad_batches': 128, 'max_epochs': 25, 'gradient_clip_val': 1.0, 'log_every_n_steps': 10, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'num_nodes': 1, 'precision': 16, 'strategy': None}, 'loader': {'batch_size': 50, 'num_workers': 0, 'pin_memory': True, 'drop_last': True}, 'dataset': {'_name_': 'wt103', 'dataset_name': 'wikitext', 'dataset_config_name': 'wikitext-103-v1', 'tokenizer_name': 'gpt2', 'cache_dir': 'dataset/wikitext-103-v1', 'max_length': 1024, 'add_eos': False, 'batch_size': 1, 'batch_size_eval': 2, 'num_workers': 4, 'shuffle': True, 'pin_memory': True}, 'optimizer': {'_name_': 'adamw', 'lr': 0.0015, 'weight_decay': 0.25, 'betas': {0: 0.9, 1: 0.999}}, 'scheduler': {'_name_': 'cosine_warmup_timm', 't_in_epochs': False, 't_initial': 22500, 'lr_min': 0.00015000000000000001, 'warmup_lr_init': 1e-06, 'warmup_t': 225.0}, 'callbacks': {'learning_rate_monitor': {'logging_interval': 'step'}, 'timer': {'step': True, 'inter_step': False, 'epoch': True, 'val': True}, 'params': {'total': True, 'trainable': True, 'fixed': True}, 'model_checkpoint': {'monitor': 'val/loss', 'mode': 'min', 'save_top_k': 1, 'save_last': True, 'dirpath': 'checkpoints/', 'filename': 'val/loss', 'auto_insert_metric_name': False, 'verbose': True}}, 'task': {'_name_': 'lm', 'loss': 'cross_entropy', 'torchmetrics': {0: 'perplexity', 1: 'num_tokens'}}, 'encoder': None, 'decoder': None, 'model': {'_name_': 'mamba_lm_pos', 'dropout': 0.25}, '_wandb': {}}
2025-02-07 17:33:35,638 INFO    MainThread:29620 [wandb_init.py:init():789] starting backend
2025-02-07 17:33:35,995 INFO    MainThread:29620 [wandb_init.py:init():793] sending inform_init request
2025-02-07 17:33:36,024 INFO    MainThread:29620 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=spawn, using: spawn
2025-02-07 17:33:36,026 INFO    MainThread:29620 [wandb_init.py:init():808] backend started and connected
2025-02-07 17:33:36,027 INFO    MainThread:29620 [wandb_init.py:init():901] updated telemetry
2025-02-07 17:33:36,028 INFO    MainThread:29620 [wandb_init.py:init():936] communicating run to backend with 90.0 second timeout
2025-02-07 17:33:37,480 INFO    MainThread:29620 [wandb_init.py:init():994] starting run threads in backend
2025-02-07 17:33:38,040 INFO    MainThread:29620 [wandb_run.py:_console_start():2385] atexit reg
2025-02-07 17:33:38,040 INFO    MainThread:29620 [wandb_run.py:_redirect():2235] redirect: wrap_raw
2025-02-07 17:33:38,040 INFO    MainThread:29620 [wandb_run.py:_redirect():2300] Wrapping output streams.
2025-02-07 17:33:38,041 INFO    MainThread:29620 [wandb_run.py:_redirect():2325] Redirects installed.
2025-02-07 17:33:38,048 INFO    MainThread:29620 [wandb_init.py:init():1036] run started, returning control to user process
2025-02-07 17:33:38,074 WARNING MsgRouterThr:29620 [router.py:message_loop():75] message_loop has been closed
